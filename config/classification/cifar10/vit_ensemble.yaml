coslr: true
endlr: 0.0
dataset:
  batch_size: 128
  dataset: cifar10
  dataroot: ./data/CIFAR10
  img_size: 224
  num_workers: 8
  use_gpu: true
networks:
  def_file: ./models/ViT_Ensemble_New.py
  params: {model_name: vit_tiny_patch16_224, num_classes: 10, selected_layers: [5, 7, 9], aux_depth: 3, normalized: false, scale: 30}

model_dir: null
optim_params: {lr: 0.1, momentum: 0.9, weight_decay: 0.0001}

training_opt:
  log_dir: ./logs/cifar10/ViT_Ensemble
  num_epochs: 200
  warmup_epoch: 5
  step1: 100
  step2: 150
  mixed_loss: true
  alpha: 1
  base_weight: 0.5
  gamma: 1
  mixer_type: mixup
